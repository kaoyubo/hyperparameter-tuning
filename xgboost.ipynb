{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "\n",
    "<br>\n",
    "\n",
    "Boosted regression trees (BRT) is a robust ML model often used in winning data science competitions, and XGBoost is an implementation of BRT that is popular due to its speed. BRT can be understood in two parts, the first part is regression trees, and second part is ensemble learning (boosting). BRT algorithm additively constructs regression trees under different conditions, then use output of all trees, often hundreds or thousands, to predict target output of new data. Note that boosting for regression trees is slightly different than boosting for classifiction trees. Boosting for regression trees requires fitting additional trees on the residual, while boosting for classification trees leads to weights increase for misclassified data points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Trees \n",
    "\n",
    "Here we show a simple BRT algorithm written from scratch to outline each of the main steps. Regular regression tree algorithm splits data repetitively based on largest decrease in loss function until stopping criteria is met.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    def __init__(self):\n",
    "        self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BRT is prone to overfitting, and there are several parameters we can tune to get best results. Based on its purpose, parameters used in XGBoost can be grouped into a few kinds:\n",
    "\n",
    "\n",
    "* **Regularization**\n",
    "\n",
    " 1. alpha: L1 regularization term on weights. \n",
    " 2. lambda: L2 regularization term on weights. \n",
    " \n",
    " \n",
    "* **Control Tree Complexity**\n",
    "\n",
    " 1. max_depth: Maximum depth of tree\n",
    " 2. min_child_weight: For regression this is minimum number of observations in each node. \n",
    " 3. gamma: Minimum loss reduction required to further split a leaf node. \n",
    " \n",
    " \n",
    "* **Randomize Training Data**\n",
    "\n",
    " 1. subsample: Subsample ratio of training observations. \n",
    " 2. colsample_bytree: Fraction of columns to be subsampled. \n",
    " \n",
    "Last but not least, **eta: Learning rate** and **n_estimators (num_round): number of trees** are very crucial for all tuning tasks. According to [AWS sagemaker doc](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-tuning.html), the parameters with greatest effects are: alpha, min_child_weight, subsample, eta, and num_round. \n",
    "\n",
    " \n",
    "Reference:\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/parameter.html\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
